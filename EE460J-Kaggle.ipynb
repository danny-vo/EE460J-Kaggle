{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install numpy pandas sklearn --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in the csv data\n",
    "train_data = pd.read_csv(\"train_final.csv\")\n",
    "test_data = pd.read_csv(\"test_final.csv\")\n",
    "# Example the contents\n",
    "# print(train_data.shape)\n",
    "# print(train_data.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y = pd.Series(train_data[\"Y\"])\n",
    "X = train_data.loc[:, \"f1\":\"f24\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict_probability() for submission\n",
    "# play with these params\n",
    "params={\n",
    "    'max_depth': [3, 5, 7, 9], #[3,4,5,6,7,8,9], # 5 is good but takes too long in kaggle env\n",
    "    'subsample': [0.4, 0.6, 0.8, .95], #[0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n",
    "    'colsample_bytree': [0.5, 0.7, 0.9], #[0.5,0.6,0.7,0.8],\n",
    "    'n_estimators': [100, 500, 1000, 2000], #[1000,2000,3000]\n",
    "    'reg_alpha': [0.01, 0.03, 0.05], #[0.01, 0.02, 0.03, 0.04],\n",
    "    'silent': [1],\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training one model took: 3.482536792755127  to run\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# B.D.E.\n",
    "A = xgb.XGBClassifier(\n",
    "    n_estimators = 1000,\n",
    "    max_depth = 7,\n",
    "    subsample = 0.8,\n",
    "    colsample_bytree = 0.7,\n",
    "    reg_alpha = 0.03,\n",
    "    silent = 1,\n",
    "    update = 'grow_gpu',\n",
    "    tree_method = 'gpu_hist',\n",
    "    predictor = 'gpu_predictor'\n",
    ")\n",
    "\n",
    "A.fit(X, Y)\n",
    "\n",
    "print(\"Training one model took: \" + str(time.time() - start_time), \" to run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost with grid search\n",
      "Begin GridSearchCV\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=20, error_score='raise-deprecating',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth={'tree_method': ['gpu_hist'], 'predictor': ['gpu_predictor'], 'updater': ['grow_gpu']},\n",
       "       min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "       nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'max_depth': [3, 5, 7, 9], 'subsample': [0.4, 0.6, 0.8, 0.95], 'colsample_bytree': [0.5, 0.7, 0.9], 'n_estimators': [100, 500, 1000, 2000], 'reg_alpha': [0.01, 0.03, 0.05], 'silent': [1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('XGBoost with grid search')\n",
    "\n",
    "xbgc_params = {\n",
    "    'tree_method': ['gpu_hist'],\n",
    "    'predictor': ['gpu_predictor'],\n",
    "    'updater': ['grow_gpu']\n",
    "}\n",
    "xgb_clf = xgb.XGBClassifier(xbgc_params)\n",
    "\n",
    "print('Begin GridSearchCV')\n",
    "rs = GridSearchCV(xgb_clf,\n",
    "                  params,\n",
    "                  cv=20,\n",
    "                  scoring=\"roc_auc\",\n",
    "                  n_jobs=-1,\n",
    "                  verbose=False)\n",
    "rs.fit(X, Y)\n",
    "# best_est = rs.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Grab results into csv for checking\n",
    "results = pd.DataFrame(rs.cv_results_)\n",
    "results.sort_values(by='rank_test_score', inplace=True)\n",
    "results.to_csv('training_results.csv', ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data.loc[:, \"f1\":\"f24\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=0.5, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=9, min_child_weight=1, missing=None, n_estimators=500,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.01, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=1, subsample=0.95)\n"
     ]
    }
   ],
   "source": [
    "best_model = rs.best_estimator_\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.predict_proba(X_test)\n",
    "formatted_predictions = np.array(predictions)\n",
    "submission = pd.DataFrame({'Id':test_data.Id, 'Y': formatted_predictions[:,1]})\n",
    "submission.to_csv('submissions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=0.5, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=9, min_child_weight=1, missing=None, n_estimators=500,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.01, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=1, subsample=0.95)\n"
     ]
    }
   ],
   "source": [
    "# Okay, lets try an average of models...\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=20, error_score='raise-deprecating',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth={'tree_method': ['gpu_hist'], 'predictor': ['gpu_predictor'], 'updater': ['grow_gpu']},\n",
       "       min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "       nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'colsample_bytree': [0.45, 0.5, 0.55], 'max_depth': [8, 9, 10], 'n_estimators': [250, 500, 750], 'reg_alpha': [0.005, 0.01, 0.015], 'subsample': [0.95], 'silent': [1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=False)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get variables from cv results, grab actual parameters\n",
    "\n",
    "# Create 5 sets of parameters (max_depth[X,X-1,X+1], subsample +- .05, colsample +- .05,\n",
    "# n_estimators ((+- half of values), reg_alpha _- .05))\n",
    "\n",
    "# Fine tune each of the 5 for n_estimators\n",
    "# Take the test values, run them through all 5 models, take an average, submit and win\n",
    "model1_ftparams ={\n",
    "    'colsample_bytree': [0.45, 0.5, 0.55],\n",
    "    'max_depth': [8, 9, 10],  \n",
    "    'n_estimators': [250, 500, 750],\n",
    "    'reg_alpha': [0.005, 0.01, 0.015],\n",
    "    'subsample': [0.95],\n",
    "    'silent': [1],\n",
    "}\n",
    "\n",
    "ftgrid_1 = GridSearchCV(xgb_clf,\n",
    "                  model1_ftparams,\n",
    "                  cv=20,\n",
    "                  scoring=\"roc_auc\",\n",
    "                  n_jobs=-1,\n",
    "                  verbose=False)\n",
    "ftgrid_1.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=20, error_score='raise-deprecating',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth={'tree_method': ['gpu_hist'], 'predictor': ['gpu_predictor'], 'updater': ['grow_gpu']},\n",
       "       min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "       nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'colsample_bytree': [0.45, 0.5, 0.55], 'max_depth': [8, 9, 10], 'n_estimators': [500, 1000, 1500], 'reg_alpha': [0.005, 0.01, 0.015], 'subsample': [0.95], 'silent': [1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=False)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3_ftparams ={\n",
    "    'colsample_bytree': [0.45, 0.5, 0.55],\n",
    "    'max_depth': [8, 9, 10],  \n",
    "    'n_estimators': [500, 1000, 1500],\n",
    "    'reg_alpha': [0.005, 0.01, 0.015],\n",
    "    'subsample': [0.95],\n",
    "    'silent': [1],\n",
    "}\n",
    "\n",
    "ftgrid_3 = GridSearchCV(xgb_clf,\n",
    "                  model3_ftparams,\n",
    "                  cv=20,\n",
    "                  scoring=\"roc_auc\",\n",
    "                  n_jobs=-1,\n",
    "                  verbose=False)\n",
    "ftgrid_3.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=20, error_score='raise-deprecating',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth={'tree_method': ['gpu_hist'], 'predictor': ['gpu_predictor'], 'updater': ['grow_gpu']},\n",
       "       min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "       nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'colsample_bytree': [0.45, 0.5, 0.55], 'max_depth': [8, 9, 10], 'n_estimators': [250, 500, 750], 'reg_alpha': [0.045, 0.05, 0.055], 'subsample': [0.95], 'silent': [1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=False)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4_ftparams ={\n",
    "    'colsample_bytree': [0.45, 0.5, 0.55],\n",
    "    'max_depth': [8, 9, 10],  \n",
    "    'n_estimators': [250, 500, 750],\n",
    "    'reg_alpha': [0.045, 0.05, 0.055],\n",
    "    'subsample': [0.95],\n",
    "    'silent': [1],\n",
    "}\n",
    "\n",
    "ftgrid_4 = GridSearchCV(xgb_clf,\n",
    "                  model4_ftparams,\n",
    "                  cv=20,\n",
    "                  scoring=\"roc_auc\",\n",
    "                  n_jobs=-1,\n",
    "                  verbose=False)\n",
    "ftgrid_4.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=20, error_score='raise-deprecating',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth={'tree_method': ['gpu_hist'], 'predictor': ['gpu_predictor'], 'updater': ['grow_gpu']},\n",
       "       min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "       nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'colsample_bytree': [0.45, 0.5, 0.55], 'max_depth': [8, 9, 10], 'n_estimators': [500, 1000, 1500], 'reg_alpha': [0.025, 0.03, 0.035], 'subsample': [0.95], 'silent': [1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=False)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model8_ftparams ={\n",
    "    'colsample_bytree': [0.45, 0.5, 0.55],\n",
    "    'max_depth': [8, 9, 10],  \n",
    "    'n_estimators': [500, 1000, 1500],\n",
    "    'reg_alpha': [0.025, 0.03, 0.035],\n",
    "    'subsample': [0.95],\n",
    "    'silent': [1],\n",
    "}\n",
    "\n",
    "ftgrid_8 = GridSearchCV(xgb_clf,\n",
    "                  model8_ftparams,\n",
    "                  cv=20,\n",
    "                  scoring=\"roc_auc\",\n",
    "                  n_jobs=-1,\n",
    "                  verbose=False)\n",
    "ftgrid_8.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vod/.local/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/vod/.local/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/vod/.local/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/vod/.local/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/vod/.local/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/vod/.local/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split5_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/vod/.local/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split6_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/vod/.local/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split7_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/vod/.local/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split8_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/vod/.local/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split9_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/vod/.local/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split10_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/vod/.local/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split11_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/vod/.local/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split12_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/vod/.local/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split13_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/vod/.local/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split14_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/vod/.local/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split15_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/vod/.local/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split16_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/vod/.local/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split17_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/vod/.local/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split18_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/vod/.local/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split19_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/vod/.local/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/vod/.local/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "ftgrid_1_results = pd.DataFrame(ftgrid_1.cv_results_)\n",
    "ftgrid_1_results.sort_values(by='rank_test_score', inplace=True)\n",
    "ftgrid_1_results.to_csv('model1_ft.csv', ',')\n",
    "ftgrid_3_results = pd.DataFrame(ftgrid_3.cv_results_)\n",
    "ftgrid_3_results.sort_values(by='rank_test_score', inplace=True)\n",
    "ftgrid_3_results.to_csv('model3_ft.csv', ',')\n",
    "ftgrid_4_results = pd.DataFrame(ftgrid_4.cv_results_)\n",
    "ftgrid_4_results.sort_values(by='rank_test_score', inplace=True)\n",
    "ftgrid_4_results.to_csv('model4_ft.csv', ',')\n",
    "ftgrid_8_results = pd.DataFrame(ftgrid_8.cv_results_)\n",
    "ftgrid_8_results.sort_values(by='rank_test_score', inplace=True)\n",
    "ftgrid_8_results.to_csv('model8_ft.csv', ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_ft_predictions = np.array(ftgrid_1.best_estimator_.predict_proba(X_test))[:,1]\n",
    "model3_ft_predictions = np.array(ftgrid_3.best_estimator_.predict_proba(X_test))[:,1]\n",
    "model4_ft_predictions = np.array(ftgrid_4.best_estimator_.predict_proba(X_test))[:,1]\n",
    "model8_ft_predictions = np.array(ftgrid_8.best_estimator_.predict_proba(X_test))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_ft_predictions = (model1_ft_predictions + model3_ft_predictions + model4_ft_predictions + model8_ft_predictions) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9159833  0.97035176 0.9999771  ... 0.9998962  0.9978585  0.9926251 ]\n"
     ]
    }
   ],
   "source": [
    "print(avg_ft_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'Id':test_data.Id, 'Y': avg_ft_predictions})\n",
    "submission.to_csv('submissions1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, let's try to avoid overfitting now, calibrate learning curve\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
